{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e43e7169",
   "metadata": {},
   "source": [
    "# Q1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a42a121",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "88640634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "264df36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the webedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "23ef7826",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com/'\n",
    "#now we want to open a window on the url by webdriver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fa744252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Data Analyst” in “Skill, Designations, Companies” field\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div[1]/input\")\n",
    "search_field_designation.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "76a79467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Bangalore” in “enter the location” field\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_field_location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9d1edef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "94a35e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create 4 empty lists.In this lists the data will be stored while webscraping.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "06a7fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_title.\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_title' list\n",
    "for i in title_tags:\n",
    "    T=i.text\n",
    "    job_title.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bb75c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and Data Analyst',\n",
       " 'SAS Analyst / data Analyst / Business analyst - Sas + SQL',\n",
       " 'Data Analyst I',\n",
       " 'Senior Data Analyst',\n",
       " 'Senior Data Analyst, Data & Analytics',\n",
       " 'Hiring - Zonal Data Analyst (Off Role) - Operations - Bangalore & Delh',\n",
       " 'Senior Data Analyst',\n",
       " 'Sr Domain Expert -Data Analysts',\n",
       " 'Data Analyst Sr',\n",
       " 'Data Analyst II']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8923002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_location.\n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_location' list\n",
    "for i in location_tags:\n",
    "    L=i.text\n",
    "    job_location.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f61bbf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'New Delhi, Bangalore/Bengaluru',\n",
       " 'Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4bcd43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the company_name\n",
    "company_name_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'company_name' list\n",
    "for i in company_name_tags:\n",
    "    C=i.text\n",
    "    company_name.append(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "113e391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CAREERDOST ENTERPRISE',\n",
       " 'Leading US MNC into analytics',\n",
       " 'Bioclinica',\n",
       " 'Shell',\n",
       " 'ReSource Pro Operational Solutions Pvt Ltd.',\n",
       " 'Rupeek',\n",
       " 'LatentView',\n",
       " 'Siemens',\n",
       " 'Epsilon',\n",
       " 'Cerner']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "97ee3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the experience_required\n",
    "experience_required_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'experience_required' list\n",
    "for i in experience_required_tags:\n",
    "    E=i.text\n",
    "    experience_required.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b2244d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-5 Yrs',\n",
       " '2-7 Yrs',\n",
       " '0-3 Yrs',\n",
       " '10-12 Yrs',\n",
       " '3-5 Yrs',\n",
       " '2-7 Yrs',\n",
       " '3-6 Yrs',\n",
       " '0-10 Yrs',\n",
       " '5-7 Yrs',\n",
       " '6-10 Yrs']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0b6eb",
   "metadata": {},
   "source": [
    "#now we have extracted the required data from the webpage and stored in the 4 lists mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "19961965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# we check the length of each list\n",
    "print(len(job_title),len(job_location),len(company_name),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "06fb249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a dataframe for the first 10 jobs related scraped data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job_title']=job_title[0:10]\n",
    "jobs['job_location']=job_location[0:10]\n",
    "jobs['company_name']=company_name[0:10]\n",
    "jobs['experience_required']=experience_required[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "ecb497c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>CAREERDOST ENTERPRISE</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SAS Analyst / data Analyst / Business analyst ...</td>\n",
       "      <td>Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...</td>\n",
       "      <td>Leading US MNC into analytics</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst I</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Bioclinica</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Shell</td>\n",
       "      <td>10-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Analyst, Data &amp; Analytics</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>ReSource Pro Operational Solutions Pvt Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hiring - Zonal Data Analyst (Off Role) - Opera...</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>Rupeek</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru</td>\n",
       "      <td>LatentView</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sr Domain Expert -Data Analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>0-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst Sr</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Epsilon</td>\n",
       "      <td>5-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst II</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Cerner</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                          Business and Data Analyst   \n",
       "1  SAS Analyst / data Analyst / Business analyst ...   \n",
       "2                                     Data Analyst I   \n",
       "3                                Senior Data Analyst   \n",
       "4              Senior Data Analyst, Data & Analytics   \n",
       "5  Hiring - Zonal Data Analyst (Off Role) - Opera...   \n",
       "6                                Senior Data Analyst   \n",
       "7                    Sr Domain Expert -Data Analysts   \n",
       "8                                    Data Analyst Sr   \n",
       "9                                    Data Analyst II   \n",
       "\n",
       "                                        job_location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1  Bangalore/Bengaluru, Delhi / NCR, Mumbai (All ...   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                     New Delhi, Bangalore/Bengaluru   \n",
       "6                       Chennai, Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                  company_name experience_required  \n",
       "0                        CAREERDOST ENTERPRISE             0-5 Yrs  \n",
       "1                Leading US MNC into analytics             2-7 Yrs  \n",
       "2                                   Bioclinica             0-3 Yrs  \n",
       "3                                        Shell           10-12 Yrs  \n",
       "4  ReSource Pro Operational Solutions Pvt Ltd.             3-5 Yrs  \n",
       "5                                       Rupeek             2-7 Yrs  \n",
       "6                                   LatentView             3-6 Yrs  \n",
       "7                                      Siemens            0-10 Yrs  \n",
       "8                                      Epsilon             5-7 Yrs  \n",
       "9                                       Cerner            6-10 Yrs  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "cec43a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653edb7",
   "metadata": {},
   "source": [
    "# Q2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4619092",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "897899b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b9e81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the webedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9636e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.naukri.com/'\n",
    "#now we want to open a window on the url by webdriver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38defc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Data Scientist” in “Skill, Designations, Companies” field\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div[1]/input\")\n",
    "search_field_designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cf2aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Bangalore” in “enter the location” field\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_field_location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9a88ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d62019ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create 4 empty lists.In this lists the data will be stored while webscraping.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7e3c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_title.\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_title' list\n",
    "for i in title_tags:\n",
    "    T=i.text\n",
    "    job_title.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ee73bf67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist',\n",
       " 'Manager - Data Scientist Job @ Applied Materials - Bangalore',\n",
       " 'Data Scientist 2',\n",
       " 'Senior Data Scientist',\n",
       " 'Senior Data Scientist/Data Scientist - Research/Analytics/Consulting',\n",
       " 'Data Scientist',\n",
       " 'Immediate requirement of Data Scientist ( Bangalore)',\n",
       " 'Senior Data Scientist']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "19424970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_location.\n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_location' list\n",
    "for i in location_tags:\n",
    "    L=i.text\n",
    "    job_location.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b9b4fb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(Whitefield)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "254bf4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the company_name\n",
    "company_name_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'company_name' list\n",
    "for i in company_name_tags:\n",
    "    C=i.text\n",
    "    company_name.append(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b65a5369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Applied Materials',\n",
       " 'Applied Materials',\n",
       " 'Applied Materials',\n",
       " 'Applied Materials',\n",
       " 'PayPal',\n",
       " 'Flipkart',\n",
       " 'Hexaconcepts',\n",
       " 'GSK India',\n",
       " 'Firstsource Solutions Limited',\n",
       " 'GSK India']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f60ccacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the experience_required\n",
    "experience_required_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'experience_required' list\n",
    "for i in experience_required_tags:\n",
    "    E=i.text\n",
    "    experience_required.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09df37a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-3 Yrs',\n",
       " '4-7 Yrs',\n",
       " '2-7 Yrs',\n",
       " '10-15 Yrs',\n",
       " '7-10 Yrs',\n",
       " '5-8 Yrs',\n",
       " '6-9 Yrs',\n",
       " '4-7 Yrs',\n",
       " '2-7 Yrs',\n",
       " '5-9 Yrs']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c522c",
   "metadata": {},
   "source": [
    "#now we have extracted the required data from the webpage and stored in the 4 lists mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "67d09a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# we check the length of each list\n",
    "print(len(job_title),len(job_location),len(company_name),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8b8290a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a dataframe for the first 10 jobs related scraped data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job_title']=job_title[0:10]\n",
    "jobs['job_location']=job_location[0:10]\n",
    "jobs['company_name']=company_name[0:10]\n",
    "jobs['experience_required']=experience_required[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d23ff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manager - Data Scientist Job @ Applied Materia...</td>\n",
       "      <td>Bangalore/Bengaluru(Whitefield)</td>\n",
       "      <td>Applied Materials</td>\n",
       "      <td>10-15 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist 2</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist/Data Scientist - Researc...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Hexaconcepts</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Immediate requirement of Data Scientist ( Bang...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Firstsource Solutions Limited</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GSK India</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0                                     Data Scientist   \n",
       "1                                     Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3  Manager - Data Scientist Job @ Applied Materia...   \n",
       "4                                   Data Scientist 2   \n",
       "5                              Senior Data Scientist   \n",
       "6  Senior Data Scientist/Data Scientist - Researc...   \n",
       "7                                     Data Scientist   \n",
       "8  Immediate requirement of Data Scientist ( Bang...   \n",
       "9                              Senior Data Scientist   \n",
       "\n",
       "                      job_location                   company_name  \\\n",
       "0              Bangalore/Bengaluru              Applied Materials   \n",
       "1              Bangalore/Bengaluru              Applied Materials   \n",
       "2              Bangalore/Bengaluru              Applied Materials   \n",
       "3  Bangalore/Bengaluru(Whitefield)              Applied Materials   \n",
       "4              Bangalore/Bengaluru                         PayPal   \n",
       "5              Bangalore/Bengaluru                       Flipkart   \n",
       "6              Bangalore/Bengaluru                   Hexaconcepts   \n",
       "7              Bangalore/Bengaluru                      GSK India   \n",
       "8              Bangalore/Bengaluru  Firstsource Solutions Limited   \n",
       "9              Bangalore/Bengaluru                      GSK India   \n",
       "\n",
       "  experience_required  \n",
       "0             0-3 Yrs  \n",
       "1             4-7 Yrs  \n",
       "2             2-7 Yrs  \n",
       "3           10-15 Yrs  \n",
       "4            7-10 Yrs  \n",
       "5             5-8 Yrs  \n",
       "6             6-9 Yrs  \n",
       "7             4-7 Yrs  \n",
       "8             2-7 Yrs  \n",
       "9             5-9 Yrs  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40a51f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closed webdriver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ae4d9c",
   "metadata": {},
   "source": [
    "# Q3: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3029b",
   "metadata": {},
   "source": [
    "In this question you have to scrape data using the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bc3883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "759f84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the webedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dee3bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d1e9256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Data Scientist” in “Skill, Designations, Companies” field\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[1]/div/div/div[1]/input\")\n",
    "search_field_designation.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0db8c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Bangalore” in “enter the location” field\n",
    "search_field_location=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[3]/div/div/div/input\")\n",
    "search_field_location.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "545d9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div[2]/div[3]/div/div/div[6]\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eebc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using \"Delhi/NCR\" on location filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "27d59b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[1]/div[2]/div[4]/label/p/span[1]\")\n",
    "location_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24d80458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using \"3-6laks\" on salary filter\n",
    "salary_filter=driver.find_element_by_xpath(\"/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3121fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After using filter we create 4 empty lists.In this lists the data will be stored while webscraping.\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cb1fa86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_title.\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_title' list\n",
    "for i in title_tags:\n",
    "    T=i.text\n",
    "    job_title.append(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6492392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist/ Machine Learning, 2022 Passout Can also apply',\n",
       " 'Data Scientist',\n",
       " 'Data Scientist - Python/SQL',\n",
       " 'Data Scientist',\n",
       " 'Urgent opening For Data Scientist role',\n",
       " 'Data & Applied Scientist',\n",
       " 'Data and applied Scientist',\n",
       " 'Data Scientist',\n",
       " 'Senior Data Scientist Lead',\n",
       " 'Data Analyst/Data Engineer']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2798db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the job_location.\n",
    "location_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span[1]\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'job_location' list\n",
    "for i in location_tags:\n",
    "    L=i.text\n",
    "    job_location.append(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "da61e888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hyderabad/Secunderabad, Ahmedabad, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Gurgaon, Bengaluru',\n",
       " 'Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra, Gurgaon/Gurugram, Jaipur, Bangalore/Bengaluru',\n",
       " 'Bharuch, Jaipur, Bhopal, Mumbai, Jhansi, Nagpur, Ghaziabad, Jaunpur, Kanpur, Delhi, Lucknow, Agra, Gurgaon, Rajkot, Bengaluru',\n",
       " 'New Delhi, Bangalore/Bengaluru',\n",
       " 'Noida, Hyderabad/Secunderabad, Bangalore/Bengaluru',\n",
       " 'Noida, Hyderabad/Secunderabad, Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'Kolkata, Hyderabad/Secunderabad, Pune, Chennai, Bangalore/Bengaluru, Delhi / NCR, Mumbai (All Areas)',\n",
       " 'Noida, Mumbai, Hyderabad/Secunderabad, Pune, Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bb5e44ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the company_name\n",
    "company_name_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in created 'company_name' list\n",
    "for i in company_name_tags:\n",
    "    C=i.text\n",
    "    company_name.append(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3c7d18c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Creative Hands HR Consultancy',\n",
       " 'BlackBuck',\n",
       " 'AVE-Promagne',\n",
       " 'Country Veggie',\n",
       " 'GLOBAL UPSIDE India Pvt Ltd',\n",
       " 'Microsoft',\n",
       " 'Microsoft',\n",
       " 'SpotDraft',\n",
       " 'Wipro',\n",
       " 'Coforge']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1f963c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the tags where we have the experience_required\n",
    "experience_required_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "# extract the text from these tags one by one by looping over these tags and stored in created 'experience_required' list\n",
    "for i in experience_required_tags:\n",
    "    E=i.text\n",
    "    experience_required.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d5d43e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0-4 Yrs',\n",
       " '3-7 Yrs',\n",
       " '3-8 Yrs',\n",
       " '1-3 Yrs',\n",
       " '2-4 Yrs',\n",
       " '3-7 Yrs',\n",
       " '3-7 Yrs',\n",
       " '1-5 Yrs',\n",
       " '6-11 Yrs',\n",
       " '6-10 Yrs']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience_required[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eac7e0",
   "metadata": {},
   "source": [
    "#now we have extracted the required data from the webpage and stored in the 4 lists mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf8cbe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "# we check the length of each list\n",
    "print(len(job_title),len(job_location),len(company_name),len(company_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "631db84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we create a dataframe for the first 10 jobs related scraped data\n",
    "jobs=pd.DataFrame({})\n",
    "jobs['job_title']=job_title[0:10]\n",
    "jobs['job_location']=job_location[0:10]\n",
    "jobs['company_name']=company_name[0:10]\n",
    "jobs['experience_required']=experience_required[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ee6c772e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist/ Machine Learning, 2022 Passout...</td>\n",
       "      <td>Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...</td>\n",
       "      <td>Creative Hands HR Consultancy</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon, Bengaluru</td>\n",
       "      <td>BlackBuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist - Python/SQL</td>\n",
       "      <td>Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra...</td>\n",
       "      <td>AVE-Promagne</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bharuch, Jaipur, Bhopal, Mumbai, Jhansi, Nagpu...</td>\n",
       "      <td>Country Veggie</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Urgent opening For Data Scientist role</td>\n",
       "      <td>New Delhi, Bangalore/Bengaluru</td>\n",
       "      <td>GLOBAL UPSIDE India Pvt Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data &amp; Applied Scientist</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Bangalore/Benga...</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data and applied Scientist</td>\n",
       "      <td>Noida, Hyderabad/Secunderabad, Bangalore/Benga...</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>SpotDraft</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist Lead</td>\n",
       "      <td>Kolkata, Hyderabad/Secunderabad, Pune, Chennai...</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst/Data Engineer</td>\n",
       "      <td>Noida, Mumbai, Hyderabad/Secunderabad, Pune, B...</td>\n",
       "      <td>Coforge</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title  \\\n",
       "0  Data Scientist/ Machine Learning, 2022 Passout...   \n",
       "1                                     Data Scientist   \n",
       "2                        Data Scientist - Python/SQL   \n",
       "3                                     Data Scientist   \n",
       "4             Urgent opening For Data Scientist role   \n",
       "5                           Data & Applied Scientist   \n",
       "6                         Data and applied Scientist   \n",
       "7                                     Data Scientist   \n",
       "8                         Senior Data Scientist Lead   \n",
       "9                         Data Analyst/Data Engineer   \n",
       "\n",
       "                                        job_location  \\\n",
       "0  Hyderabad/Secunderabad, Ahmedabad, Chennai, Ba...   \n",
       "1                                 Gurgaon, Bengaluru   \n",
       "2  Noida, Kota, Mumbai, Chandigarh, Lucknow, Agra...   \n",
       "3  Bharuch, Jaipur, Bhopal, Mumbai, Jhansi, Nagpu...   \n",
       "4                     New Delhi, Bangalore/Bengaluru   \n",
       "5  Noida, Hyderabad/Secunderabad, Bangalore/Benga...   \n",
       "6  Noida, Hyderabad/Secunderabad, Bangalore/Benga...   \n",
       "7              Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "8  Kolkata, Hyderabad/Secunderabad, Pune, Chennai...   \n",
       "9  Noida, Mumbai, Hyderabad/Secunderabad, Pune, B...   \n",
       "\n",
       "                    company_name experience_required  \n",
       "0  Creative Hands HR Consultancy             0-4 Yrs  \n",
       "1                      BlackBuck             3-7 Yrs  \n",
       "2                   AVE-Promagne             3-8 Yrs  \n",
       "3                 Country Veggie             1-3 Yrs  \n",
       "4    GLOBAL UPSIDE India Pvt Ltd             2-4 Yrs  \n",
       "5                      Microsoft             3-7 Yrs  \n",
       "6                      Microsoft             3-7 Yrs  \n",
       "7                      SpotDraft             1-5 Yrs  \n",
       "8                          Wipro            6-11 Yrs  \n",
       "9                        Coforge            6-10 Yrs  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finaly we created the required DataFrame\n",
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a40fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closed webdrive\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22f14d",
   "metadata": {},
   "source": [
    "# Q4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1958c79",
   "metadata": {},
   "source": [
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands andmore” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this pageyou can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom ofthe page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a393972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "600fbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bcfec40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we get the webpage:https://www.flipkart.com/\n",
    "url=\" https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e4c623cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid the login page\n",
    "login_page=driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "login_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "db80834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find “Sunglasses” in “products,brands and more” field\n",
    "search_field_product=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "search_field_product.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "27ae7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5875bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after going to the sunglasses page,we create 3 empty lists.In this lists the data will be stored while webscraping.\n",
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "02d7829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brand tags of sunglasses\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in brand list\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c66c0e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2d251c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the product description tags of sunglasses\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "#extract the text from these tags one by one by looping over these tags and stored in product_description list\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c16f000f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "040dcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the price tags of sunglasses\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in price list\n",
    "for i in price_tags:\n",
    "    p=i.text\n",
    "    price.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "94e6d94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1be425ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have to extract 100 sungasses data,we are going to the next page by next button click\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "174922d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brand tags of next page of sunglasses\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in brand list\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "1ed90fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4437ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the product description tags of next page of sunglasses\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "#extract the text from these tags one by one by looping over these tags and stored in product_description list\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "39c9c9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "234765da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the price tags of next page of sunglasses\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in price list\n",
    "for i in price_tags:\n",
    "    p=i.text\n",
    "    price.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2d5f5b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8dc43ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to extract 100 sungasses data but we extract only 80 sunglass data.so,we are going to the third page of sunglasses\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5b2e9473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brand tags of sunglasses\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in brand list\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "48043145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "275404f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all the product description tags of third page of sunglasses\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "#extract the text from these tags one by one by looping over these tags and stored in product_description list\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dc03243e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cfe599bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the price tags of third page of sunglasses\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "# extract the text from these tags one by one looping over these tags and stored in price list\n",
    "for i in price_tags:\n",
    "    p=i.text\n",
    "    price.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4575ac84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "bf0fdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extracted >100 sunglasses data.now we are making a dataframe of 100 sunglasses data\n",
    "import pandas as pd\n",
    "sunglasses=pd.DataFrame({})\n",
    "sunglasses[\"Brand\"]=brand[0:100]\n",
    "sunglasses[\"Product Description\"]=product_description[0:100]\n",
    "sunglasses[\"Price\"]=price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e4620e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Roadster</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (60)</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRYSTAL CART</td>\n",
       "      <td>Polarized, UV Protection, Gradient, Riding Gla...</td>\n",
       "      <td>₹349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Specs</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>₹249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>john jacobs</td>\n",
       "      <td>Polarized, UV Protection Retro Square Sunglass...</td>\n",
       "      <td>₹3,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (58)</td>\n",
       "      <td>₹989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored Aviator Sunglasses (57)</td>\n",
       "      <td>₹185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Riding Glasses Wayfarer Sunglas...</td>\n",
       "      <td>₹223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Cristiano Ronnie</td>\n",
       "      <td>UV Protection, Riding Glasses Aviator Sunglass...</td>\n",
       "      <td>₹449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand                                Product Description  \\\n",
       "0           Roadster         UV Protection Retro Square Sunglasses (60)   \n",
       "1       CRYSTAL CART  Polarized, UV Protection, Gradient, Riding Gla...   \n",
       "2          Elligator                UV Protection Round Sunglasses (54)   \n",
       "3          New Specs   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "4               SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "..               ...                                                ...   \n",
       "95       john jacobs  Polarized, UV Protection Retro Square Sunglass...   \n",
       "96          Fastrack             UV Protection Wayfarer Sunglasses (58)   \n",
       "97             NuVew    UV Protection, Mirrored Aviator Sunglasses (57)   \n",
       "98             NuVew  UV Protection, Riding Glasses Wayfarer Sunglas...   \n",
       "99  Cristiano Ronnie  UV Protection, Riding Glasses Aviator Sunglass...   \n",
       "\n",
       "     Price  \n",
       "0     ₹399  \n",
       "1     ₹349  \n",
       "2     ₹295  \n",
       "3     ₹269  \n",
       "4     ₹249  \n",
       "..     ...  \n",
       "95  ₹3,500  \n",
       "96    ₹989  \n",
       "97    ₹185  \n",
       "98    ₹223  \n",
       "99    ₹449  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finaly we made the required DataFrame of 100 data of sunglasses \n",
    "sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f12fc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#closed webdrive\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e095ff4f",
   "metadata": {},
   "source": [
    "# Q5: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a240a2",
   "metadata": {},
   "source": [
    "Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage .\n",
    "As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0f049cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b602b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the webdriver\n",
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90b1d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.flipkart.com/apple-iphone-12-black-128-gb/product-reviews/itmf1f0a58f1ecd7?pid=MOBFWBYZK3HACR72&lid=LSTMOBFWBYZK3HACR72PX4KSA&marketplace=FLIPKART\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03763d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating=[]\n",
    "review_summary=[]\n",
    "full_review=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1345dacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "len(rating)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3e14f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5', '5', '5', '5', '5', '5', '5', '5', '5', '5']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f652fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "len(review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0591818b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Terrific',\n",
       " 'Perfect product!',\n",
       " 'Terrific purchase',\n",
       " 'Mind-blowing purchase',\n",
       " 'Awesome',\n",
       " 'Awesome',\n",
       " 'Highly recommended',\n",
       " 'Must buy!',\n",
       " 'Brilliant',\n",
       " 'Brilliant']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7830ea19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "len(full_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54406f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wow superb camera phone\\nVery smooth speed and no lag , iphone is the king always\\nIts a beautiful product',\n",
       " 'The brand is very trustworthy and i got genuine pice at a very low cost.I ordered the green one and trust me the colour was amazing.All the colours but specifically green and purple are nice for look.Thanks to flipkart❤️',\n",
       " 'Awesome phone … value for money.. Happy with battery life.. Awesome camera features… look at the images snapped using the phone… easy to use.. Just feared about getting scratch at back glass… but using cover helped it…',\n",
       " 'Guys ,this is just Beast at Every Aspect of Configurations, Full Pack with What You want, Like Best Camera , Best Display, Best Battery for whole Day Use, And Everyone know About Processing Speed👌..',\n",
       " \"Thanks flipkart i trust you got my device perfectly loved it best phone in it's segment\",\n",
       " 'Excellent product worth for every penny, writing this review after using 7 days, earlier was using iPhone 6Plus now on iPhone 12 😍, faster then anything this else.\\nExcellent Picture quality.\\nJust loved it.!!',\n",
       " 'My 1st iPhone ever and I’m loving it. Great performance, awesome display, camera is outstanding which comes with heavily priced. But worth it. White color looks super cool. 🎉😊😍',\n",
       " 'Delightful phone, the phone is just a peice of art, sleek, eye catchy, super fast and got everything u need...best one available',\n",
       " 'It’s my first iPhone ever and I bought it with my earned money through part time jobs in college✌️\\n\\nI am a tech freak so you can trust my views -\\n- A14 Bionic is the fastest, most efficient and reliable processor till date\\n- The camera focuses so quickly that you can take DSLR quality photos.\\n- The screen size 6.1 inches is the most comfortable screen size out there and the OLED retina XDR display is so crisp and everything feels real.\\n- The stereo speakers are so clear even on high volume an...\\nREAD MORE',\n",
       " 'Excellent product worth every penny right this review after using 7 days earlier was using phone iPhone 6s now on iPhone 12😍😍 faster than anything this else… excellent picture quality just love it iPhone12']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "28654ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c97fcf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75a76c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9253c5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e5e6957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "60\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9cc9a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "70\n",
      "70\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2226f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c30f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ffdaf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0db67bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n",
      "110\n",
      "110\n"
     ]
    }
   ],
   "source": [
    "next_button_tag=driver.find_element_by_xpath(\"/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]/span\")\n",
    "next_button_tag.click()\n",
    "rating_tags=driver.find_elements_by_xpath(\"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "print(len(rating))    \n",
    "review_summery_tags=driver.find_elements_by_xpath(\"//p[@class='_2-N8zT']\")\n",
    "for i in review_summery_tags:\n",
    "    RE=i.text\n",
    "    review_summary.append(RE)\n",
    "print(len(review_summary))\n",
    "full_review_tags=driver.find_elements_by_xpath(\"//div[@class='t-ZTKy']/div[1]\")\n",
    "for i in full_review_tags:\n",
    "    F=i.text\n",
    "    full_review.append(F)\n",
    "print(len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2873d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4238dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extracted >100 iphone11 rating data.now we are making a dataframe of 100 sunglasses data\n",
    "import pandas as pd\n",
    "iphone11_review=pd.DataFrame({})\n",
    "iphone11_review[\"Rating\"]=rating[0:100]\n",
    "iphone11_review[\"Review Summery\"]=review_summary[0:100]\n",
    "iphone11_review[\"Full Review\"]=full_review[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0f66c964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summery</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific</td>\n",
       "      <td>Wow superb camera phone\\nVery smooth speed and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>The brand is very trustworthy and i got genuin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Awesome phone … value for money.. Happy with b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Guys ,this is just Beast at Every Aspect of Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>Thanks flipkart i trust you got my device perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>My final review after using it for whole one w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>Worth the money invested. My first iPhone and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>I recently upgraded to the iPhone 12 and I hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3</td>\n",
       "      <td>Pretty good</td>\n",
       "      <td>Definitely an awesome product...an awesome exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice</td>\n",
       "      <td>An apple fan, I find iPhone 12 light, easier t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating         Review Summery  \\\n",
       "0       5               Terrific   \n",
       "1       5       Perfect product!   \n",
       "2       5      Terrific purchase   \n",
       "3       5  Mind-blowing purchase   \n",
       "4       5                Awesome   \n",
       "..    ...                    ...   \n",
       "95      5              Just wow!   \n",
       "96      5              Brilliant   \n",
       "97      4                Awesome   \n",
       "98      3            Pretty good   \n",
       "99      5                   Nice   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Wow superb camera phone\\nVery smooth speed and...  \n",
       "1   The brand is very trustworthy and i got genuin...  \n",
       "2   Awesome phone … value for money.. Happy with b...  \n",
       "3   Guys ,this is just Beast at Every Aspect of Co...  \n",
       "4   Thanks flipkart i trust you got my device perf...  \n",
       "..                                                ...  \n",
       "95  My final review after using it for whole one w...  \n",
       "96  Worth the money invested. My first iPhone and ...  \n",
       "97  I recently upgraded to the iPhone 12 and I hav...  \n",
       "98  Definitely an awesome product...an awesome exp...  \n",
       "99  An apple fan, I find iPhone 12 light, easier t...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iphone11_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1659ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb15ba8",
   "metadata": {},
   "source": [
    "# Q6: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b81504",
   "metadata": {},
   "source": [
    "Scrape data for first 100 sneakers you find when you visit flipkart.com andsearch for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e79d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all Library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91d7cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5237939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link of the page open in the webdriver\n",
    "url=\"https://www.flipkart.com/?ef_id=36fc67681e3e112993d5050f924742af:G:s&s_kwcid=AL!739!10!76003876838574!76003887545054&semcmpid=sem_F1167BY7_Brand_adcenter\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fd1626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore the log in page\n",
    "ignore_login_page=driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button\")\n",
    "ignore_login_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab576c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find “sneakers” on search for products,brands and more bar on the page\n",
    "search_sneakers=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input\")\n",
    "search_sneakers.send_keys(\"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2537b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click search button\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[1]/div[1]/div[2]/div[2]/form/div/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3ab8ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create 3 empty list where the data be stored while scrapingthe data from the page\n",
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05433f8c",
   "metadata": {},
   "source": [
    "Scrape data for sneakers from the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d1947141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brands tags from the page\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extract data converting into data and stored into brand list using by for loop\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)\n",
    "#extract all the product description tags from the page\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa _2-ICcC' or @class='IRpwTa']\")\n",
    "#extract data converting into data and stored into product_description list using by for loop\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)\n",
    "#extract all the price tags from the page\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#extract data converting into data and stored into price list using by for loop\n",
    "for i in price_tags:\n",
    "    P=i.text\n",
    "    price.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7d92883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80 80\n"
     ]
    }
   ],
   "source": [
    "#check the length of the every list\n",
    "print(len(brand),len(product_description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dfe9c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to the next page by next button click\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5111b5",
   "metadata": {},
   "source": [
    "Scrape data for sneakers from the second page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dee4ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brands tags from the page\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extract data converting into data and stored into brand list using by for loop\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)\n",
    "#extract all the product description tags from the page\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa _2-ICcC' or @class='IRpwTa']\")\n",
    "#extract data converting into data and stored into product_description list using by for loop\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)\n",
    "#extract all the price tags from the page\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#extract data converting into data and stored into price list using by for loop\n",
    "for i in price_tags:\n",
    "    P=i.text\n",
    "    price.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c9e3819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 80 80\n"
     ]
    }
   ],
   "source": [
    "#check the length of the every list\n",
    "print(len(brand),len(product_description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "952b58c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to the next page by next button click\n",
    "next_button=driver.find_element_by_xpath(\"/html/body/div[1]/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]\")\n",
    "next_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8d52b",
   "metadata": {},
   "source": [
    "Scrape data for sneakers from the third page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b422af02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brands tags from the page\n",
    "brand_tags=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "#extract data converting into data and stored into brand list using by for loop\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)\n",
    "#extract all the product description tags from the page\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//a[@class='IRpwTa _2-ICcC' or @class='IRpwTa']\")\n",
    "#extract data converting into data and stored into product_description list using by for loop\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)\n",
    "#extract all the price tags from the page\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='_30jeq3']\")\n",
    "#extract data converting into data and stored into price list using by for loop\n",
    "for i in price_tags:\n",
    "    P=i.text\n",
    "    price.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "621e1691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120\n"
     ]
    }
   ],
   "source": [
    "#check the length of the every list\n",
    "print(len(brand),len(product_description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f24b9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extracted >100 sneakers data.now we are making a dataframe of 100 sunglasses data\n",
    "import pandas as pd\n",
    "sneakers=pd.DataFrame({})\n",
    "sneakers[\"Brand\"]=brand[0:100]\n",
    "sneakers[\"Product Description\"]=product_description[0:100]\n",
    "sneakers[\"Price\"]=price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e673406b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GLS</td>\n",
       "      <td>SHORT (WHITE) 10 Sneakers For Men</td>\n",
       "      <td>₹799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GLS</td>\n",
       "      <td>LONG (BLACK) 11 Sneakers For Men</td>\n",
       "      <td>₹849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D-SNEAKERZ</td>\n",
       "      <td>Casual , Partywear Sneakers Shoes For Men's An...</td>\n",
       "      <td>₹299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Sneakers Sneakers For Men</td>\n",
       "      <td>₹198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>corsac</td>\n",
       "      <td>STYLISH MENS BLACK SNEAKER Sneakers For Men</td>\n",
       "      <td>₹499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hotmess</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>RODDICK SHOES</td>\n",
       "      <td>Fashion Outdoor Canvas Casual Light Weight Lac...</td>\n",
       "      <td>₹480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ESSENCE</td>\n",
       "      <td>Trendy Sneakers For Men</td>\n",
       "      <td>₹424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Stefano Rads</td>\n",
       "      <td>Classy Sneakers For Men</td>\n",
       "      <td>₹240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>mohliye</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                                Product Description Price\n",
       "0             GLS                  SHORT (WHITE) 10 Sneakers For Men  ₹799\n",
       "1             GLS                   LONG (BLACK) 11 Sneakers For Men  ₹849\n",
       "2      D-SNEAKERZ  Casual , Partywear Sneakers Shoes For Men's An...  ₹299\n",
       "3        URBANBOX                          Sneakers Sneakers For Men  ₹198\n",
       "4          corsac        STYLISH MENS BLACK SNEAKER Sneakers For Men  ₹499\n",
       "..            ...                                                ...   ...\n",
       "95        Hotmess                                   Sneakers For Men  ₹228\n",
       "96  RODDICK SHOES  Fashion Outdoor Canvas Casual Light Weight Lac...  ₹480\n",
       "97        ESSENCE                            Trendy Sneakers For Men  ₹424\n",
       "98   Stefano Rads                            Classy Sneakers For Men  ₹240\n",
       "99        mohliye                                   Sneakers For Men  ₹299\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9fa53a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c888f2",
   "metadata": {},
   "source": [
    "# Q7: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4437469",
   "metadata": {},
   "source": [
    "Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 7149 to Rs. 14099 ” , Color filter to “Black”, as shown inthe below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe\n",
    "description, price of the shoe as shown in the below image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f066ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all Library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ceaccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e8b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link of the page open in the webdriver\n",
    "url=\"http://www.myntra.com/shoes\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7f3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Price filter to “Rs. 7149 to Rs. 14099 ”\n",
    "price_filter_tag=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div\")\n",
    "price_filter_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be9aa911",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Color filter to “Black”\n",
    "color_filter_tag=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div\")\n",
    "color_filter_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6bd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create 3 empty list where the data be stored while scrapingthe data from the page\n",
    "brand=[]\n",
    "product_description=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6fb8a",
   "metadata": {},
   "source": [
    "scrap the shoes data from the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21930804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brands tags from the page\n",
    "brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "#extract data converting into data and stored into brand list using by for loop\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)\n",
    "#extract all the product description tags from the page\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "#extract data converting into data and stored into product_description list using by for loop\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)\n",
    "#extract all the price tags from the page\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "#extract data converting into data and stored into price list using by for loop\n",
    "for i in price_tags:\n",
    "    P=i.text\n",
    "    price.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "461f9eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n"
     ]
    }
   ],
   "source": [
    "#check the length of the every list\n",
    "print(len(brand),len(product_description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3fc1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#go to the next page\n",
    "next_page_button=driver.find_element_by_xpath(\"/html/body/div[2]/div/div[1]/main/div[3]/div[2]/div/div[2]/section/div[2]/ul/li[12]\")\n",
    "next_page_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c954c",
   "metadata": {},
   "source": [
    "scrap the shoes data from the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c71fc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the brands tags from the page\n",
    "brand_tags=driver.find_elements_by_xpath(\"//h3[@class='product-brand']\")\n",
    "#extract data converting into data and stored into brand list using by for loop\n",
    "for i in brand_tags:\n",
    "    B=i.text\n",
    "    brand.append(B)\n",
    "#extract all the product description tags from the page\n",
    "product_description_tags=driver.find_elements_by_xpath(\"//h4[@class='product-product']\")\n",
    "#extract data converting into data and stored into product_description list using by for loop\n",
    "for i in product_description_tags:\n",
    "    P=i.text\n",
    "    product_description.append(P)\n",
    "#extract all the price tags from the page\n",
    "price_tags=driver.find_elements_by_xpath(\"//div[@class='product-price']\")\n",
    "#extract data converting into data and stored into price list using by for loop\n",
    "for i in price_tags:\n",
    "    P=i.text\n",
    "    price.append(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759e34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "#check the length of the every list\n",
    "print(len(brand),len(product_description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf7e72c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extracted 100 shoes data.now we are making a dataframe of 100 sunglasses data\n",
    "import pandas as pd\n",
    "shoes=pd.DataFrame({})\n",
    "shoes[\"Brand\"]=brand[0:100]\n",
    "shoes[\"Shoes Description\"]=product_description[0:100]\n",
    "shoes[\"Price\"]=price[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f81017d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Shoes Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>AIRFORCE 1'07 LV8 EMB Sneakers</td>\n",
       "      <td>Rs. 8725Rs. 9695(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>AIR FORCE 1 '07 LV8 Sneakers</td>\n",
       "      <td>Rs. 8725Rs. 9695(10% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Leather Driving Shoes</td>\n",
       "      <td>Rs. 12999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Max Cushioning Running</td>\n",
       "      <td>Rs. 7649Rs. 8999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skechers</td>\n",
       "      <td>Men Walking Shoes</td>\n",
       "      <td>Rs. 7649Rs. 8999(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>DAVINCHI</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>Rs. 9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bugatti</td>\n",
       "      <td>Men Walking Shoes</td>\n",
       "      <td>Rs. 7199Rs. 8999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Royal Enfield</td>\n",
       "      <td>Cabo WP Riding Boots</td>\n",
       "      <td>Rs. 8075Rs. 8500(5% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Men Colourblocked PU Sneakers</td>\n",
       "      <td>Rs. 7999Rs. 9999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Sole To Soul</td>\n",
       "      <td>Platform Peep Toes with Buckles</td>\n",
       "      <td>Rs. 7690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Brand                Shoes Description                      Price\n",
       "0            Nike   AIRFORCE 1'07 LV8 EMB Sneakers  Rs. 8725Rs. 9695(10% OFF)\n",
       "1            Nike     AIR FORCE 1 '07 LV8 Sneakers  Rs. 8725Rs. 9695(10% OFF)\n",
       "2            ALDO        Men Leather Driving Shoes                  Rs. 12999\n",
       "3        Skechers       Men Max Cushioning Running  Rs. 7649Rs. 8999(15% OFF)\n",
       "4        Skechers                Men Walking Shoes  Rs. 7649Rs. 8999(15% OFF)\n",
       "..            ...                              ...                        ...\n",
       "95       DAVINCHI  Men Solid Leather Formal Derbys                   Rs. 9990\n",
       "96        Bugatti                Men Walking Shoes  Rs. 7199Rs. 8999(20% OFF)\n",
       "97  Royal Enfield             Cabo WP Riding Boots   Rs. 8075Rs. 8500(5% OFF)\n",
       "98          ASICS    Men Colourblocked PU Sneakers  Rs. 7999Rs. 9999(20% OFF)\n",
       "99   Sole To Soul  Platform Peep Toes with Buckles                   Rs. 7690\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7757878",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386af691",
   "metadata": {},
   "source": [
    "# Q8:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a99f15",
   "metadata": {},
   "source": [
    "Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributesfor each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da427bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all Library\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd956cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open webdriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3856cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#link of the page open in the webdriver\n",
    "url=\"https://www.amazon.in/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd278f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Enter “Laptop” in the search field and then click the search icon.\n",
    "driver.get(\"https://www.amazon.in/s?k=Laptop&crid=YAI7F84QEFVP&sprefix=laptop%2Caps%2C415&ref=nb_sb_noss_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5721e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set CPU Type filter to “Intel Core i7” and “Intel Core i9” \n",
    "i7_filter_tag=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[4]/li[12]/span/a/div/label/i\")\n",
    "i7_filter_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ab97f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "rating=[]\n",
    "price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c685a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 28 30\n"
     ]
    }
   ],
   "source": [
    "title_tag=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in title_tag:\n",
    "    T=i.text\n",
    "    title.append(T)\n",
    "rating_tag=driver.find_elements_by_xpath(\"//span[@class='a-size-base s-underline-text']\")\n",
    "for i in rating_tag:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "price_tag=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in price_tag:\n",
    "    P=i.text\n",
    "    price.append(P)\n",
    "print(len(title),len(rating),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "155a9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=title[0:10]\n",
    "rating=rating[0:10]\n",
    "price=price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "701136e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Enter “Laptop” in the search field and then click the search icon.\n",
    "driver.get(\"https://www.amazon.in/s?k=Laptop&crid=YAI7F84QEFVP&sprefix=laptop%2Caps%2C415&ref=nb_sb_noss_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fd533f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set CPU Type filter to “Intel Core i7” and “Intel Core i9” \n",
    "i9_filter_tag=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]/ul[4]/li[13]/span/a/div/label/i\")\n",
    "i9_filter_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b38e4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 14 19\n"
     ]
    }
   ],
   "source": [
    "title_tag=driver.find_elements_by_xpath(\"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "for i in title_tag:\n",
    "    T=i.text\n",
    "    title.append(T)\n",
    "rating_tag=driver.find_elements_by_xpath(\"//span[@class='a-size-base s-underline-text']\")\n",
    "for i in rating_tag:\n",
    "    R=i.text\n",
    "    rating.append(R)\n",
    "price_tag=driver.find_elements_by_xpath(\"//span[@class='a-price-whole']\")\n",
    "for i in price_tag:\n",
    "    P=i.text\n",
    "    price.append(P)\n",
    "print(len(title),len(rating),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ec7d5636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making DataFrame\n",
    "import pandas as pd\n",
    "laptop=pd.DataFrame({})\n",
    "laptop[\"Laptop Title\"]=title[0:10]\n",
    "laptop[\"Rating\"]=rating[0:10]\n",
    "laptop[\"Price\"]=price[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fad4d6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...</td>\n",
       "      <td>680</td>\n",
       "      <td>37,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP 14s 11th Gen Intel Core i3- 8GB RAM/256GB S...</td>\n",
       "      <td>1,907</td>\n",
       "      <td>38,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lenovo IdeaPad 3 11th Gen Intel i3 15.6\" FHD T...</td>\n",
       "      <td>134</td>\n",
       "      <td>41,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...</td>\n",
       "      <td>680</td>\n",
       "      <td>37,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HP 14s 11th Gen Intel Core i3- 8GB RAM/256GB S...</td>\n",
       "      <td>1,907</td>\n",
       "      <td>38,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dell New Vostro 3510 Laptop, Intel i3-1005G1, ...</td>\n",
       "      <td>658</td>\n",
       "      <td>38,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dell New Inspiron 3511 Laptop Intel i3-1115G4 ...</td>\n",
       "      <td>6</td>\n",
       "      <td>39,190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS VivoBook 14 (2021), Intel Core i3-1115G4 ...</td>\n",
       "      <td>62</td>\n",
       "      <td>35,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...</td>\n",
       "      <td>170</td>\n",
       "      <td>35,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...</td>\n",
       "      <td>146</td>\n",
       "      <td>36,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Laptop Title Rating   Price\n",
       "0  Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...    680  37,990\n",
       "1  HP 14s 11th Gen Intel Core i3- 8GB RAM/256GB S...  1,907  38,990\n",
       "2  Lenovo IdeaPad 3 11th Gen Intel i3 15.6\" FHD T...    134  41,990\n",
       "3  Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...    680  37,990\n",
       "4  HP 14s 11th Gen Intel Core i3- 8GB RAM/256GB S...  1,907  38,990\n",
       "5  Dell New Vostro 3510 Laptop, Intel i3-1005G1, ...    658  38,990\n",
       "6  Dell New Inspiron 3511 Laptop Intel i3-1115G4 ...      6  39,190\n",
       "7  ASUS VivoBook 14 (2021), Intel Core i3-1115G4 ...     62  35,990\n",
       "8  ASUS VivoBook 14 (2021), 14-inch (35.56 cms) F...    170  35,990\n",
       "9  Lenovo IdeaPad Slim 3 10th Gen Intel Core i3 1...    146  36,990"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laptop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17bbefe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6293f75c",
   "metadata": {},
   "source": [
    "# Q9: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d1d09e",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for first 10 job results for Data Scientist Designation in Noida\n",
    "location. You have to scrape company name, No. of days ago when job was posted, Rating of the company.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the Job option as shown in the image\n",
    "3. After reaching to the next webpage, In place of “Search by Designations, Companies, Skills” enter\n",
    "“Data Scientist” and click on search button.\n",
    "4. You will reach to the following web page click on location and in place of “Search location” enter\n",
    "“Noida” and select location “Noida”.\n",
    "5. Then scrape the data for the first 10 jobs results you get on the above shown page.\n",
    "6. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f519dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d74b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the webedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c111bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.ambitionbox.com/'\n",
    "#now we want to open a webpage on the url by webdriver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "984dda4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on job option\n",
    "job_option_tags=driver.find_element_by_xpath(\"/html/body/div[1]/nav/nav/a[6]\")\n",
    "job_option_tags.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a9b26461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search by Designations, Companies, Skills” enter “Data Scientist” and click on search button.\n",
    "search_field_designation=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/div/span/input\")\n",
    "search_field_designation.send_keys('Data Scientist')\n",
    "search_button=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[1]/div/div/div/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3eea0385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#click on location\n",
    "location_tag=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]\")\n",
    "location_tag.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ce114d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in place of “Search location” enter “Noida”\n",
    "search_tag=driver.find_element_by_xpath(\"/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[2]/input\")\n",
    "search_tag.send_keys(\"Noida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "907f0191",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath('/html/body/div/div/div/div[2]/div[1]/div[2]/div[1]/div/div/div/div[2]/div[2]/div/div[3]/div[1]/div[1]/div/label')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "26272ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we create 3 empty lists.In this lists the data will be stored while webscraping.\n",
    "rating=[]\n",
    "days=[]\n",
    "company_name=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1b6816dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape company name, No. of days ago when job was posted, Rating of the company and stored into the 3 above created list\n",
    "company_name_tags=driver.find_elements_by_xpath(\"//div[@class='company-info']/p[1]\")\n",
    "for i in company_name_tags:\n",
    "    C=i.text\n",
    "    company_name.append(C)\n",
    "days_tags=driver.find_elements_by_xpath(\"//div[@class='other-info']/span[1]\")\n",
    "for i in days_tags:\n",
    "    D=i.text\n",
    "    days.append(D)\n",
    "rating_tags=driver.find_elements_by_xpath(\"//span[@class='body-small']\")\n",
    "for i in rating_tags:\n",
    "    R=i.text\n",
    "    rating.append(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b846f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(company_name),len(days),len(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7817bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' · ',\n",
       " 'GENPACT India Private Limited',\n",
       " 'GENPACT India Private Limited',\n",
       " 'Tech Mahindra Ltd',\n",
       " 'GENPACT India Private Limited',\n",
       " 'HCL Technologies',\n",
       " 'Zyoin',\n",
       " 'Newgen Software Technologies Ltd.',\n",
       " 'Pitney Bowes India Pvt Ltd',\n",
       " 'JK Technosoft Ltd',\n",
       " 'Microsoft India (R and D) Pvt Ltd']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6a01e0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1d ago',\n",
       " '6d ago',\n",
       " '20d ago',\n",
       " '23d ago',\n",
       " '27d ago',\n",
       " '5d ago',\n",
       " '7d ago',\n",
       " '1mon ago',\n",
       " '16d ago',\n",
       " '2mon ago']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a1f0e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.0', '4.0', '3.7', '4.0', '3.8', '4.1', '3.5', '4.2', '3.6', '4.3']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8860570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we extracted 10 job results for Data Scientist Designation in Noida location\n",
    "import pandas as pd\n",
    "jobs=pd.DataFrame({})\n",
    "jobs[\"Company Name\"]=company_name[1:11]\n",
    "jobs[\"No. of days ago when job was posted\"]=days[0:10]\n",
    "jobs[\"Rating\"]=rating[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d368dec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>No. of days ago when job was posted</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>1d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>6d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tech Mahindra Ltd</td>\n",
       "      <td>20d ago</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GENPACT India Private Limited</td>\n",
       "      <td>23d ago</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HCL Technologies</td>\n",
       "      <td>27d ago</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zyoin</td>\n",
       "      <td>5d ago</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Newgen Software Technologies Ltd.</td>\n",
       "      <td>7d ago</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pitney Bowes India Pvt Ltd</td>\n",
       "      <td>1mon ago</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JK Technosoft Ltd</td>\n",
       "      <td>16d ago</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Microsoft India (R and D) Pvt Ltd</td>\n",
       "      <td>2mon ago</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Company Name No. of days ago when job was posted  \\\n",
       "0      GENPACT India Private Limited                              1d ago   \n",
       "1      GENPACT India Private Limited                              6d ago   \n",
       "2                  Tech Mahindra Ltd                             20d ago   \n",
       "3      GENPACT India Private Limited                             23d ago   \n",
       "4                   HCL Technologies                             27d ago   \n",
       "5                              Zyoin                              5d ago   \n",
       "6  Newgen Software Technologies Ltd.                              7d ago   \n",
       "7         Pitney Bowes India Pvt Ltd                            1mon ago   \n",
       "8                  JK Technosoft Ltd                             16d ago   \n",
       "9  Microsoft India (R and D) Pvt Ltd                            2mon ago   \n",
       "\n",
       "  Rating  \n",
       "0    4.0  \n",
       "1    4.0  \n",
       "2    3.7  \n",
       "3    4.0  \n",
       "4    3.8  \n",
       "5    4.1  \n",
       "6    3.5  \n",
       "7    4.2  \n",
       "8    3.6  \n",
       "9    4.3  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b651e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6f6df",
   "metadata": {},
   "source": [
    "# Q10: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797023c4",
   "metadata": {},
   "source": [
    "Write a python program to scrape the salary data for Data Scientist designation.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Minsalary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. First get the webpage https://www.ambitionbox.com/\n",
    "2. Click on the salaries option as shown in the image.\n",
    "3. After reaching to the following webpage, In place of “Search Job Profile” enters “Data Scientist” and\n",
    "then click on “Data Scientist”.\n",
    "You have to scrape the data ticked in the above image.\n",
    "4. Scrape the data for the first 10 companies. Scrape the company name, total salary record, average\n",
    "salary, minimum salary, maximum salary, experience required.\n",
    "5. Store the data in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5331ca02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all library\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4feb3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to the webedriver\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd24d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://www.ambitionbox.com/\"\n",
    "#now we want to open a webpage on the url by webdriver\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2040b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lick on the salaries option \n",
    "salary_tags=driver.find_element_by_xpath('/html/body/div[1]/nav/nav/a[4]')\n",
    "salary_tags.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa95d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search Job Profile” enters “Data Scientist” and then click on “Data Scientist”\n",
    "job_profile_tag=driver.find_element_by_xpath('/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/input')\n",
    "job_profile_tag.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88abc72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button_tags=driver.find_element_by_xpath(\"/html/body/div/div/div/main/section[1]/div[2]/div[1]/span/div/div/div[1]\")\n",
    "search_button_tags.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d42cf0e",
   "metadata": {},
   "source": [
    "creating 6 empty list to stored the company name, total salary record, average salary, minimum salary, maximum salary, experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60da1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name=[]\n",
    "salary_record=[]\n",
    "avg_salary=[]\n",
    "min_salary=[]\n",
    "max_salary=[]\n",
    "experience=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ddbd9",
   "metadata": {},
   "source": [
    " Scrape the company name, total salary record, average salary, minimum salary, maximum salary, experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5090bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name_tags=driver.find_elements_by_xpath(\"//div[@class='company-info']\")\n",
    "for i in company_name_tags:\n",
    "    C=i.text\n",
    "    company_name.append(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c21cba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walmart\\nbased on 11 salaries\\nData Scientist\\n . \\n3 yrs exp',\n",
       " 'Ab Inbev\\nbased on 32 salaries\\nData Scientist\\n . \\n3-4 yrs exp',\n",
       " 'Reliance Jio\\nbased on 10 salaries\\nData Scientist\\n . \\n4 yrs exp',\n",
       " 'ZS\\nbased on 15 salaries\\nData Scientist\\n . \\n2 yrs exp',\n",
       " 'Optum\\nbased on 27 salaries\\nData Scientist\\n . \\n3-4 yrs exp',\n",
       " 'Fractal Analytics\\nbased on 81 salaries\\nData Scientist\\n . \\n2-4 yrs exp',\n",
       " 'Tiger Analytics\\nbased on 46 salaries\\nData Scientist\\n . \\n2-4 yrs exp',\n",
       " 'UnitedHealth\\nbased on 53 salaries\\nData Scientist\\n . \\n2-4 yrs exp',\n",
       " 'Verizon\\nbased on 14 salaries\\nData Scientist\\n . \\n4 yrs exp',\n",
       " 'Ganit Business Solutions\\nbased on 13 salaries\\nData Scientist\\n . \\n4 yrs exp',\n",
       " 'TCS\\n3.1\\nSalary Rating \\n34.2k reviews',\n",
       " 'Cognizant\\n3.4\\nSalary Rating \\n22.7k reviews',\n",
       " 'Accenture\\n3.6\\nSalary Rating \\n24.9k reviews',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38d0cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_record_tags=driver.find_elements_by_xpath(\"//div[@class='name']\")\n",
    "for i in salary_record_tags:\n",
    "    S=i.text\n",
    "    salary_record.append(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05cb8db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walmart\\nbased on 11 salaries',\n",
       " 'Ab Inbev\\nbased on 32 salaries',\n",
       " 'Reliance Jio\\nbased on 10 salaries',\n",
       " 'ZS\\nbased on 15 salaries',\n",
       " 'Optum\\nbased on 27 salaries',\n",
       " 'Fractal Analytics\\nbased on 81 salaries',\n",
       " 'Tiger Analytics\\nbased on 46 salaries',\n",
       " 'UnitedHealth\\nbased on 53 salaries',\n",
       " 'Verizon\\nbased on 14 salaries',\n",
       " 'Ganit Business Solutions\\nbased on 13 salaries']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0067aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary=[]\n",
    "for i in salary_record:\n",
    "    m=i.split()[0]\n",
    "    salary.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac83114b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Walmart',\n",
       " 'Ab',\n",
       " 'Reliance',\n",
       " 'ZS',\n",
       " 'Optum',\n",
       " 'Fractal',\n",
       " 'Tiger',\n",
       " 'UnitedHealth',\n",
       " 'Verizon',\n",
       " 'Ganit']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66904e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_salary_tags=driver.find_elements_by_xpath(\"//p[@class='averageCtc']\")\n",
    "for i in avg_salary_tags:\n",
    "    A=i.text\n",
    "    avg_salary.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b24df536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 29.7L',\n",
       " '₹ 20.5L',\n",
       " '₹ 18.9L',\n",
       " '₹ 15.9L',\n",
       " '₹ 15.2L',\n",
       " '₹ 15.2L',\n",
       " '₹ 14.8L',\n",
       " '₹ 14.0L',\n",
       " '₹ 12.7L',\n",
       " '₹ 12.4L']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cfbc652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salary=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a16c6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_salary_tags=driver.find_elements_by_xpath(\"//div[@class='value body-medium']\")\n",
    "for i in min_salary_tags:\n",
    "    min=i.text\n",
    "    min_salary.append(min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b1e1c347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 25.0L',\n",
       " '₹ 35.0L',\n",
       " '₹ 15.0L',\n",
       " '₹ 25.5L',\n",
       " '₹ 5.6L',\n",
       " '₹ 26.2L',\n",
       " '₹ 9.8L',\n",
       " '₹ 20.0L',\n",
       " '₹ 11.0L',\n",
       " '₹ 22.0L',\n",
       " '₹ 9.5L',\n",
       " '₹ 22.0L',\n",
       " '₹ 9.0L',\n",
       " '₹ 20.0L',\n",
       " '₹ 8.3L',\n",
       " '₹ 20.5L',\n",
       " '₹ 10.0L',\n",
       " '₹ 21.0L',\n",
       " '₹ 8.5L',\n",
       " '₹ 15.0L']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8384f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "while(i<11):\n",
    "    min_salary.pop(i)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf23604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 25.0L',\n",
       " '₹ 15.0L',\n",
       " '₹ 5.6L',\n",
       " '₹ 9.8L',\n",
       " '₹ 11.0L',\n",
       " '₹ 9.5L',\n",
       " '₹ 9.0L',\n",
       " '₹ 8.3L',\n",
       " '₹ 10.0L',\n",
       " '₹ 8.5L']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8c8a7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_salary=[]\n",
    "max_salary_tags=driver.find_elements_by_xpath(\"//div[@class='value body-medium']\")\n",
    "for i in max_salary_tags:\n",
    "    max=i.text\n",
    "    max_salary.append(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7262265b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 25.0L',\n",
       " '₹ 35.0L',\n",
       " '₹ 15.0L',\n",
       " '₹ 25.5L',\n",
       " '₹ 5.6L',\n",
       " '₹ 26.2L',\n",
       " '₹ 9.8L',\n",
       " '₹ 20.0L',\n",
       " '₹ 11.0L',\n",
       " '₹ 22.0L',\n",
       " '₹ 9.5L',\n",
       " '₹ 22.0L',\n",
       " '₹ 9.0L',\n",
       " '₹ 20.0L',\n",
       " '₹ 8.3L',\n",
       " '₹ 20.5L',\n",
       " '₹ 10.0L',\n",
       " '₹ 21.0L',\n",
       " '₹ 8.5L',\n",
       " '₹ 15.0L']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3c881e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_salary\n",
    "i=0\n",
    "while(i<10):\n",
    "    max_salary.pop(i)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1d4e7449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['₹ 35.0L',\n",
       " '₹ 25.5L',\n",
       " '₹ 26.2L',\n",
       " '₹ 20.0L',\n",
       " '₹ 22.0L',\n",
       " '₹ 22.0L',\n",
       " '₹ 20.0L',\n",
       " '₹ 20.5L',\n",
       " '₹ 21.0L',\n",
       " '₹ 15.0L']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd459c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_tags=driver.find_elements_by_xpath(\"//div[@class='salaries sbold-list-header']\")\n",
    "for i in experience_tags:\n",
    "    E=i.text\n",
    "    experience.append(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3118ad88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data Scientist\\n . \\n3 yrs exp',\n",
       " 'Data Scientist\\n . \\n3-4 yrs exp',\n",
       " 'Data Scientist\\n . \\n4 yrs exp',\n",
       " 'Data Scientist\\n . \\n2 yrs exp',\n",
       " 'Data Scientist\\n . \\n3-4 yrs exp',\n",
       " 'Data Scientist\\n . \\n2-4 yrs exp',\n",
       " 'Data Scientist\\n . \\n2-4 yrs exp',\n",
       " 'Data Scientist\\n . \\n2-4 yrs exp',\n",
       " 'Data Scientist\\n . \\n4 yrs exp',\n",
       " 'Data Scientist\\n . \\n4 yrs exp']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a4b3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp=[]\n",
    "for i in experience:\n",
    "    ex=i.replace('Data Scientist','').replace('.','').replace('\\n','')\n",
    "    exp.append(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0588323f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  3 yrs exp',\n",
       " '  3-4 yrs exp',\n",
       " '  4 yrs exp',\n",
       " '  2 yrs exp',\n",
       " '  3-4 yrs exp',\n",
       " '  2-4 yrs exp',\n",
       " '  2-4 yrs exp',\n",
       " '  2-4 yrs exp',\n",
       " '  4 yrs exp',\n",
       " '  4 yrs exp']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d578a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "after scraping the required data ,making a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "310f28ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({})\n",
    "df['Company Name']=company_name[0:10]\n",
    "df['Total Salary Record']=salary[0:10]\n",
    "df['Average Salary']=avg_salary[0:10]\n",
    "df['Minimum Salary']=min_salary[0:10]\n",
    "df['Maximum Salary']=max_salary[0:10]\n",
    "df['Experiencr Required']=exp[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f75c751e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Total Salary Record</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Experiencr Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walmart\\nbased on 11 salaries\\nData Scientist\\...</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>₹ 29.7L</td>\n",
       "      <td>₹ 25.0L</td>\n",
       "      <td>₹ 35.0L</td>\n",
       "      <td>3 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab Inbev\\nbased on 32 salaries\\nData Scientist...</td>\n",
       "      <td>Ab</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>₹ 25.5L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliance Jio\\nbased on 10 salaries\\nData Scien...</td>\n",
       "      <td>Reliance</td>\n",
       "      <td>₹ 18.9L</td>\n",
       "      <td>₹ 5.6L</td>\n",
       "      <td>₹ 26.2L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZS\\nbased on 15 salaries\\nData Scientist\\n . \\...</td>\n",
       "      <td>ZS</td>\n",
       "      <td>₹ 15.9L</td>\n",
       "      <td>₹ 9.8L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>2 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Optum\\nbased on 27 salaries\\nData Scientist\\n ...</td>\n",
       "      <td>Optum</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "      <td>₹ 11.0L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>3-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fractal Analytics\\nbased on 81 salaries\\nData ...</td>\n",
       "      <td>Fractal</td>\n",
       "      <td>₹ 15.2L</td>\n",
       "      <td>₹ 9.5L</td>\n",
       "      <td>₹ 22.0L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Tiger Analytics\\nbased on 46 salaries\\nData Sc...</td>\n",
       "      <td>Tiger</td>\n",
       "      <td>₹ 14.8L</td>\n",
       "      <td>₹ 9.0L</td>\n",
       "      <td>₹ 20.0L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UnitedHealth\\nbased on 53 salaries\\nData Scien...</td>\n",
       "      <td>UnitedHealth</td>\n",
       "      <td>₹ 14.0L</td>\n",
       "      <td>₹ 8.3L</td>\n",
       "      <td>₹ 20.5L</td>\n",
       "      <td>2-4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Verizon\\nbased on 14 salaries\\nData Scientist\\...</td>\n",
       "      <td>Verizon</td>\n",
       "      <td>₹ 12.7L</td>\n",
       "      <td>₹ 10.0L</td>\n",
       "      <td>₹ 21.0L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ganit Business Solutions\\nbased on 13 salaries...</td>\n",
       "      <td>Ganit</td>\n",
       "      <td>₹ 12.4L</td>\n",
       "      <td>₹ 8.5L</td>\n",
       "      <td>₹ 15.0L</td>\n",
       "      <td>4 yrs exp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Company Name Total Salary Record  \\\n",
       "0  Walmart\\nbased on 11 salaries\\nData Scientist\\...             Walmart   \n",
       "1  Ab Inbev\\nbased on 32 salaries\\nData Scientist...                  Ab   \n",
       "2  Reliance Jio\\nbased on 10 salaries\\nData Scien...            Reliance   \n",
       "3  ZS\\nbased on 15 salaries\\nData Scientist\\n . \\...                  ZS   \n",
       "4  Optum\\nbased on 27 salaries\\nData Scientist\\n ...               Optum   \n",
       "5  Fractal Analytics\\nbased on 81 salaries\\nData ...             Fractal   \n",
       "6  Tiger Analytics\\nbased on 46 salaries\\nData Sc...               Tiger   \n",
       "7  UnitedHealth\\nbased on 53 salaries\\nData Scien...        UnitedHealth   \n",
       "8  Verizon\\nbased on 14 salaries\\nData Scientist\\...             Verizon   \n",
       "9  Ganit Business Solutions\\nbased on 13 salaries...               Ganit   \n",
       "\n",
       "  Average Salary Minimum Salary Maximum Salary Experiencr Required  \n",
       "0        ₹ 29.7L        ₹ 25.0L        ₹ 35.0L           3 yrs exp  \n",
       "1        ₹ 20.5L        ₹ 15.0L        ₹ 25.5L         3-4 yrs exp  \n",
       "2        ₹ 18.9L         ₹ 5.6L        ₹ 26.2L           4 yrs exp  \n",
       "3        ₹ 15.9L         ₹ 9.8L        ₹ 20.0L           2 yrs exp  \n",
       "4        ₹ 15.2L        ₹ 11.0L        ₹ 22.0L         3-4 yrs exp  \n",
       "5        ₹ 15.2L         ₹ 9.5L        ₹ 22.0L         2-4 yrs exp  \n",
       "6        ₹ 14.8L         ₹ 9.0L        ₹ 20.0L         2-4 yrs exp  \n",
       "7        ₹ 14.0L         ₹ 8.3L        ₹ 20.5L         2-4 yrs exp  \n",
       "8        ₹ 12.7L        ₹ 10.0L        ₹ 21.0L           4 yrs exp  \n",
       "9        ₹ 12.4L         ₹ 8.5L        ₹ 15.0L           4 yrs exp  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6dfb6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
